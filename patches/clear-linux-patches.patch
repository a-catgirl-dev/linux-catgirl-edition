# Patch0104: 0104-pci-pme-wakeups.patch
# Patch0108: 0108-smpboot-reuse-timer-calibration.patch
# Patch0111: 0111-ipv4-tcp-allow-the-memory-tuning-for-tcp-to-go-a-lit.patch
# Patch0116: 0116-migrate-some-systemd-defaults-to-the-kernel-defaults.patch
# Patch0117: 0117-xattr-allow-setting-user.-attributes-on-symlinks-by-.patch
# Patch0128: 0128-itmt_epb-use-epb-to-scale-itmt.patch
# Patch0130: 0130-itmt2-ADL-fixes.patch
# Patch0131: 0131-add-a-per-cpu-minimum-high-watermark-an-tune-batch-s.patch
# Patch0135: 0135-initcall-only-print-non-zero-initcall-debug-to-speed.patch
# Patch0137: libsgrowdown.patch
# Patch0141: epp-retune.patch
# Patch0148: 0002-sched-core-add-some-branch-hints-based-on-gcov-analy.patch
# Patch0150: 0150-select-core_sys_select-add-unlikely-branch-hint-on-r.patch
# Patch0157: scale-net-alloc.patch
# Patch0158: 0158-clocksource-only-perform-extended-clocksource-checks.patch
# Patch0164: 0164-KVM-VMX-make-vmx-init-a-late-init-call-to-get-to-ini.patch
# Patch0165: slack.patch
# Patch0166: 0166-sched-fair-remove-upper-limit-on-cpu-number.patch
# Patch0167: 0167-net-sock-increase-default-number-of-_SK_MEM_PACKETS-.patch
# Patch0173: 0173-cpuidle-psd-add-power-sleep-demotion-prevention-for-.patch
# Patch0174: 0174-memcg-increase-MEMCG_CHARGE_BATCH-to-128.patch

# Allow disabling the LDT (local descriptor table).

diff -uar a/arch/x86/Kconfig b/arch/x86/Kconfig
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -1239,7 +1239,7 @@
 	default X86_LEGACY_VM86
 
 config X86_16BIT
-	bool "Enable support for 16-bit segments" if EXPERT
+	bool "Enable support for 16-bit segments"
 	default y
 	depends on MODIFY_LDT_SYSCALL
 	help
@@ -2334,7 +2334,7 @@
 	  be set to 'N' under normal conditions.
 
 config MODIFY_LDT_SYSCALL
-	bool "Enable the LDT (local descriptor table)" if EXPERT
+	bool "Enable the LDT (local descriptor table)"
 	default y
 	help
 	  Linux can allow user programs to install a per-process x86

# Allow disabling legacy 16-bit UID syscall wrappers.

diff -uar a/init/Kconfig b/init/Kconfig
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -1603,7 +1603,7 @@
 	  Only use this if you really know what you are doing.
 
 config UID16
-	bool "Enable 16-bit UID system calls" if EXPERT
+	bool "Enable 16-bit UID system calls"
 	depends on HAVE_UID16 && MULTIUSER
 	default y
 	help

# Disable more debug knobs.

diff -uar a/lib/Kconfig.debug b/lib/Kconfig.debug
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -910,7 +910,7 @@
 	  regions to be regularly checked for invalid topology.
 
 config DEBUG_MEMORY_INIT
-	bool "Debug memory initialisation" if EXPERT
+	bool "Debug memory initialisation"
 	default !EXPERT
 	help
 	  Enable this for additional checks during memory initialisation.

diff -uar a/mm/Kconfig.debug b/mm/Kconfig.debug
--- a/mm/Kconfig.debug
+++ b/mm/Kconfig.debug
@@ -47,7 +47,7 @@
 
 config SLUB_DEBUG
 	default y
-	bool "Enable SLUB debugging support" if EXPERT
+	bool "Enable SLUB debugging support"
 	depends on SYSFS && !SLUB_TINY
 	select STACKDEPOT if STACKTRACE_SUPPORT
 	help

# Patch0104: 0104-pci-pme-wakeups.patch

Subject: [PATCH] pci pme wakeups

Reduce wakeups for PME checks, which are a workaround for miswired
boards (sadly, too many of them) in laptops.
---
 drivers/pci/pci.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/drivers/pci/pci.c b/drivers/pci/pci.c
index d25122fbe98a..dbfb6aaa4a07 100644
--- a/drivers/pci/pci.c
+++ b/drivers/pci/pci.c
@@ -60,7 +60,7 @@ struct pci_pme_device {
 	struct pci_dev *dev;
 };
 
-#define PME_TIMEOUT 1000 /* How long between PME checks */
+#define PME_TIMEOUT 4000 /* How long between PME checks */
 
 /*
  * Following exit from Conventional Reset, devices must be ready within 1 sec
-- 
https://clearlinux.org

# Patch0108: 0108-smpboot-reuse-timer-calibration.patch

Subject: [PATCH] smpboot: reuse timer calibration

NO point recalibrating for known-constant tsc ...
saves 200ms+ of boot time.
---
 arch/x86/kernel/tsc.c | 3 +++
 1 file changed, 3 insertions(+)

diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c
index a698196377be..5f3ee7c31c8a 100644
--- a/arch/x86/kernel/tsc.c
+++ b/arch/x86/kernel/tsc.c
@@ -1596,6 +1596,9 @@ unsigned long calibrate_delay_is_known(void)
 	if (!constant_tsc || !mask)
 		return 0;
 
+	if (cpu != 0)
+		return cpu_data(0).loops_per_jiffy;
+
 	sibling = cpumask_any_but(mask, cpu);
 	if (sibling < nr_cpu_ids)
 		return cpu_data(sibling).loops_per_jiffy;
-- 
https://clearlinux.org

# Patch0111: 0111-ipv4-tcp-allow-the-memory-tuning-for-tcp-to-go-a-lit.patch

Subject: [PATCH] ipv4/tcp: allow the memory tuning for tcp to go a little
 bigger than default

---
 net/ipv4/tcp.c | 4 ++--
 1 file changed, 2 insertions(+), 2 deletions(-)

diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c
index 28ff2a820f7c..c4f240da8d70 100644
--- a/net/ipv4/tcp.c
+++ b/net/ipv4/tcp.c
@@ -5196,8 +5196,8 @@ void __init tcp_init(void)
 	tcp_init_mem();
 	/* Set per-socket limits to no more than 1/128 the pressure threshold */
 	limit = nr_free_buffer_pages() << (PAGE_SHIFT - 7);
-	max_wshare = min(4UL*1024*1024, limit);
-	max_rshare = min(6UL*1024*1024, limit);
+	max_wshare = min(16UL*1024*1024, limit);
+	max_rshare = min(16UL*1024*1024, limit);
 
 	init_net.ipv4.sysctl_tcp_wmem[0] = PAGE_SIZE;
 	init_net.ipv4.sysctl_tcp_wmem[1] = 16*1024;
-- 
https://clearlinux.org

# Patch0116: 0116-migrate-some-systemd-defaults-to-the-kernel-defaults.patch

Subject: [PATCH] migrate some systemd defaults to the kernel defaults.

These settings are needed to prevent networking issues when
the networking modules come up by default without explicit
settings, which breaks some cases.

We don't want the modprobe settings to be read at boot time
if we're not going to do anything else ever.
---
 drivers/net/dummy.c             | 2 +-
 include/uapi/linux/if_bonding.h | 2 +-
 2 files changed, 2 insertions(+), 2 deletions(-)

diff --git a/drivers/net/dummy.c b/drivers/net/dummy.c
index f82ad7419508..5e8faa70aad6 100644
--- a/drivers/net/dummy.c
+++ b/drivers/net/dummy.c
@@ -44,7 +44,7 @@
 
 #define DRV_NAME	"dummy"
 
-static int numdummies = 1;
+static int numdummies = 0;
 
 /* fake multicast ability */
 static void set_multicast_list(struct net_device *dev)
diff --git a/include/uapi/linux/if_bonding.h b/include/uapi/linux/if_bonding.h
index d174914a837d..bf8e2af101a3 100644
--- a/include/uapi/linux/if_bonding.h
+++ b/include/uapi/linux/if_bonding.h
@@ -82,7 +82,7 @@
 #define BOND_STATE_ACTIVE       0   /* link is active */
 #define BOND_STATE_BACKUP       1   /* link is backup */
 
-#define BOND_DEFAULT_MAX_BONDS  1   /* Default maximum number of devices to support */
+#define BOND_DEFAULT_MAX_BONDS  0   /* Default maximum number of devices to support */
 
 #define BOND_DEFAULT_TX_QUEUES 16   /* Default number of tx queues per device */
 
-- 
https://clearlinux.org

# Patch0117: 0117-xattr-allow-setting-user.-attributes-on-symlinks-by-.patch

Subject: [PATCH] xattr: allow setting user.* attributes on symlinks by owner

Kvmtool and clear containers supports using user attributes to label host
files with the virtual uid/guid of the file in the container. This allows an
end user to manage their files and a complete uid space without all the ugly
namespace stuff.

The one gap in the support is symlinks because an end user can change the
ownership of a symbolic link. We support attributes on these files as you
can already (as root) set security attributes on them.

The current rules seem slightly over-paranoid and as we have a use case this
patch enables updating the attributes on a symbolic link IFF you are the
owner of the synlink (as permissions are not usually meaningful on the link
itself).

Signed-off-by: Alan Cox <alan@xxxxxxxxxx>
---
 fs/xattr.c | 15 ++++++++-------
 1 file changed, 8 insertions(+), 7 deletions(-)

diff --git a/fs/xattr.c b/fs/xattr.c
index 998045165916..62b6fb4dedee 100644
--- a/fs/xattr.c
+++ b/fs/xattr.c
@@ -139,16 +139,17 @@ xattr_permission(struct user_namespace *mnt_userns, struct inode *inode,
 	}
 
 	/*
-	 * In the user.* namespace, only regular files and directories can have
-	 * extended attributes. For sticky directories, only the owner and
-	 * privileged users can write attributes.
+	 * In the user.* namespace, only regular files, symbolic links, and
+	 * directories can have extended attributes. For symbolic links and
+	 * sticky directories, only the owner and privileged users can write
+	 * attributes.
 	 */
 	if (!strncmp(name, XATTR_USER_PREFIX, XATTR_USER_PREFIX_LEN)) {
-		if (!S_ISREG(inode->i_mode) && !S_ISDIR(inode->i_mode))
+		if (!S_ISREG(inode->i_mode) && !S_ISDIR(inode->i_mode) && !S_ISLNK(inode->i_mode))
 			return (mask & MAY_WRITE) ? -EPERM : -ENODATA;
-		if (S_ISDIR(inode->i_mode) && (inode->i_mode & S_ISVTX) &&
-		    (mask & MAY_WRITE) &&
-		    !inode_owner_or_capable(idmap, inode))
+		if (((S_ISDIR(inode->i_mode) && (inode->i_mode & S_ISVTX))
+		        || S_ISLNK(inode->i_mode)) && (mask & MAY_WRITE)
+		    && !inode_owner_or_capable(idmap, inode))
 			return -EPERM;
 	}
 
-- 
https://clearlinux.org

# Patch0128: 0128-itmt_epb-use-epb-to-scale-itmt.patch

Subject: [PATCH] itmt_epb: use epb to scale itmt

---
 arch/x86/include/asm/topology.h |  1 +
 arch/x86/kernel/cpu/intel_epb.c |  4 ++++
 arch/x86/kernel/itmt.c          | 29 ++++++++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/arch/x86/include/asm/topology.h b/arch/x86/include/asm/topology.h
index 2f0b6be8eaab..c31f81e2ea05 100644
--- a/arch/x86/include/asm/topology.h
+++ b/arch/x86/include/asm/topology.h
@@ -255,6 +255,7 @@ extern unsigned int __read_mostly sysctl_sched_itmt_enabled;
 
 /* Interface to set priority of a cpu */
 void sched_set_itmt_core_prio(int prio, int core_cpu);
+void sched_set_itmt_power_ratio(int power_ratio, int core_cpu);
 
 /* Interface to notify scheduler that system supports ITMT */
 int sched_set_itmt_support(void);
diff --git a/arch/x86/kernel/cpu/intel_epb.c b/arch/x86/kernel/cpu/intel_epb.c
index fbaf12e43f41..c8c2d6f1a8ac 100644
--- a/arch/x86/kernel/cpu/intel_epb.c
+++ b/arch/x86/kernel/cpu/intel_epb.c
@@ -166,6 +166,10 @@ static ssize_t energy_perf_bias_store(struct device *dev,
 	if (ret < 0)
 		return ret;
 
+	/* update the ITMT scheduler logic to use the power policy data */
+	/* scale the val up by 2 so the range is 224 - 256 */
+	sched_set_itmt_power_ratio(256 - val * 2, cpu);
+
 	return count;
 }
 
diff --git a/arch/x86/kernel/itmt.c b/arch/x86/kernel/itmt.c
index 57027bfed25f..596fd7fb7847 100644
--- a/arch/x86/kernel/itmt.c
+++ b/arch/x86/kernel/itmt.c
@@ -26,6 +26,7 @@
 
 static DEFINE_MUTEX(itmt_update_mutex);
 DEFINE_PER_CPU_READ_MOSTLY(int, sched_core_priority);
+DEFINE_PER_CPU_READ_MOSTLY(int, sched_power_ratio);
 
 /* Boolean to track if system has ITMT capabilities */
 static bool __read_mostly sched_itmt_capable;
@@ -144,7 +145,12 @@ void sched_clear_itmt_support(void)
 
 int arch_asym_cpu_priority(int cpu)
 {
-	return per_cpu(sched_core_priority, cpu);
+	int power_ratio = per_cpu(sched_power_ratio, cpu);
+
+	/* a power ratio of 0 (uninitialized) is assumed to be maximum */
+	if (power_ratio == 0)
+		power_ratio = 256 - 2 * 6;
+	return per_cpu(sched_core_priority, cpu) * power_ratio / 256;
 }
 
 /**
@@ -165,3 +171,24 @@ void sched_set_itmt_core_prio(int prio, int core_cpu)
 {
 	per_cpu(sched_core_priority, cpu) = prio;
 }
+
+/**
+ * sched_set_itmt_power_ratio() - Set CPU priority based on ITMT
+ * @power_ratio:	The power scaling ratio [1..256] for the core
+ * @core_cpu:		The cpu number associated with the core
+ *
+ * Set a scaling to the cpu performance based on long term power
+ * settings (like EPB).
+ *
+ * Note this is for the policy not for the actual dynamic frequency;
+ * the frequency will increase itself as workloads run on a core.
+ */
+
+void sched_set_itmt_power_ratio(int power_ratio, int core_cpu)
+{
+	int cpu;
+
+	for_each_cpu(cpu, topology_sibling_cpumask(core_cpu)) {
+		per_cpu(sched_power_ratio, cpu) = power_ratio;
+	}
+}
-- 
https://clearlinux.org

# Patch0130: 0130-itmt2-ADL-fixes.patch

Subject: [PATCH] itmt2 ADL fixes

On systems with overclocking enabled, CPPC Highest Performance can be
hard coded to 0xff. In this case even if we have cores with different
highest performance, ITMT can't be enabled as the current implementation
depends on CPPC Highest Performance.

On such systems we can use MSR_HWP_CAPABILITIES maximum performance field
when CPPC.Highest Performance is 0xff.

Due to legacy reasons, we can't solely depend on MSR_HWP_CAPABILITIES as
in some older systems CPPC Highest Performance is the only way to identify
different performing cores.

Signed-off-by: Srinivas Pandruvada <srinivas.pandruvada@xxxxxxxxxx>
---
 drivers/cpufreq/intel_pstate.c | 7 +++++++
 1 file changed, 7 insertions(+)

diff --git a/drivers/cpufreq/intel_pstate.c b/drivers/cpufreq/intel_pstate.c
index bc7f7e6759bd..ee33ad7f6f28 100644
--- a/drivers/cpufreq/intel_pstate.c
+++ b/drivers/cpufreq/intel_pstate.c
@@ -377,6 +377,13 @@ static void intel_pstate_set_itmt_prio(int cpu)
 	 * update them at any time after it has been called.
 	 */
 	sched_set_itmt_core_prio(cppc_perf.highest_perf, cpu);
+	/*
+	 * On some systems with overclocking enabled, CPPC.highest_perf is hardcoded to 0xff.
+	 * In this case we can't use CPPC.highest_perf to enable ITMT.
+	 * In this case we can look at MSR_HWP_CAPABILITIES bits [8:0] to decide.
+	 */
+	if (cppc_perf.highest_perf == 0xff)
+		cppc_perf.highest_perf = HWP_HIGHEST_PERF(READ_ONCE(all_cpu_data[cpu]->hwp_cap_cached));
 
 	if (max_highest_perf <= min_highest_perf) {
 		if (cppc_perf.highest_perf > max_highest_perf)
-- 
https://clearlinux.org

# Patch0131: 0131-add-a-per-cpu-minimum-high-watermark-an-tune-batch-s.patch

Subject: [PATCH] add a per cpu minimum high watermark an tune batch size

make sure there's at least 1024 per cpu pages... a reasonably small
amount for todays system
---
 mm/page_alloc.c | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index e6f211dcf82e..0ea48434ac7d 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -5636,11 +5636,11 @@ static int zone_batchsize(struct zone *zone)
 
 	/*
 	 * The number of pages to batch allocate is either ~0.1%
-	 * of the zone or 1MB, whichever is smaller. The batch
+	 * of the zone or 4MB, whichever is smaller. The batch
 	 * size is striking a balance between allocation latency
 	 * and zone lock contention.
 	 */
-	batch = min(zone_managed_pages(zone) >> 10, SZ_1M / PAGE_SIZE);
+	batch = min(zone_managed_pages(zone) >> 10, 4 * SZ_1M / PAGE_SIZE);
 	batch /= 4;		/* We effectively *= 4 below */
 	if (batch < 1)
 		batch = 1;
-- 
https://clearlinux.org

# Patch0135: 0135-initcall-only-print-non-zero-initcall-debug-to-speed.patch

Subject: [PATCH] initcall: only print non-zero initcall debug to speed up boot

Printing initcall timings that successfully return after 0 usecs
provides not much useful information and takes a small amount of time
to do so. Disable the initcall timings for these specific cases. On
an Alderlake i9-12900 this reduces kernel boot time by 0.67% (timed
up to the invocation of systemd starting) based on 10 boot measurements.

Signed-off-by: Colin Ian King <colin.king@xxxxxxxxxx>
---
 init/main.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/init/main.c b/init/main.c
index aa21add5f7c5..715d57f58895 100644
--- a/init/main.c
+++ b/init/main.c
@@ -1208,10 +1208,13 @@ static __init_or_module void
 trace_initcall_finish_cb(void *data, initcall_t fn, int ret)
 {
 	ktime_t rettime, *calltime = data;
+	long long delta;
 
 	rettime = ktime_get();
-	printk(KERN_DEBUG "initcall %pS returned %d after %lld usecs\n",
-		 fn, ret, (unsigned long long)ktime_us_delta(rettime, *calltime));
+	delta = ktime_us_delta(rettime, *calltime);
+	if (ret || delta)
+		printk(KERN_DEBUG "initcall %pS returned %d after %lld usecs\n",
+			fn, ret, (unsigned long long)ktime_us_delta(rettime, *calltime));
 }
 
 static ktime_t initcall_calltime;
-- 
2.39.1

# Patch0137: libsgrowdown.patch

Place libraries right below the binary for PIE binaries, this helps code locality
(and thus performance).

--- linux-5.18.2/fs/binfmt_elf.c~	2022-06-06 06:49:00.000000000 +0000
+++ linux-5.18.2/fs/binfmt_elf.c	2022-08-10 13:53:04.878633166 +0000
@@ -1293,6 +1293,8 @@
 	mm = current->mm;
 	mm->end_code = end_code;
 	mm->start_code = start_code;
+	if (start_code >= ELF_ET_DYN_BASE)
+		mm->mmap_base = start_code;
 	mm->start_data = start_data;
 	mm->end_data = end_data;
 	mm->start_stack = bprm->p;

# Patch0141: epp-retune.patch

--- linux-6.1/arch/x86/include/asm/msr-index.h~	2022-12-11 22:15:18.000000000 +0000
+++ linux-6.1/arch/x86/include/asm/msr-index.h	2022-12-16 01:31:32.266119875 +0000
@@ -554,7 +554,7 @@
 #define HWP_MAX_PERF(x) 		((x & 0xff) << 8)
 #define HWP_DESIRED_PERF(x)		((x & 0xff) << 16)
 #define HWP_ENERGY_PERF_PREFERENCE(x)	(((unsigned long long) x & 0xff) << 24)
-#define HWP_EPP_PERFORMANCE		0x00
+#define HWP_EPP_PERFORMANCE		0x01
 #define HWP_EPP_BALANCE_PERFORMANCE	0x80
 #define HWP_EPP_BALANCE_POWERSAVE	0xC0
 #define HWP_EPP_POWERSAVE		0xFF

# Patch0148: 0002-sched-core-add-some-branch-hints-based-on-gcov-analy.patch

Subject: [PATCH] sched/core: add some branch hints based on gcov analysis

Signed-off-by: Colin Ian King <colin.king@xxxxxxxxxx>
---
 kernel/sched/core.c | 8 ++++----
 1 file changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index f730b6fe94a7..ee0ec4ab7d1c 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -601,7 +601,7 @@ void raw_spin_rq_lock_nested(struct rq *rq, int subclass)
 
 	/* Matches synchronize_rcu() in __sched_core_enable() */
 	preempt_disable();
-	if (sched_core_disabled()) {
+	if (likely(sched_core_disabled())) {
 		raw_spin_lock_nested(&rq->__lock, subclass);
 		/* preempt_count *MUST* be > 1 */
 		preempt_enable_no_resched();
@@ -815,7 +815,7 @@ void update_rq_clock(struct rq *rq)
 	scx_rq_clock_update(rq, clock);
 
 	delta = clock - rq->clock;
-	if (delta < 0)
+	if (unlikely(delta < 0))
 		return;
 	rq->clock += delta;
 
@@ -6124,7 +6124,7 @@ pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 	struct rq *rq_i;
 	bool need_sync;
 
-	if (!sched_core_enabled(rq))
+	if (likely(!sched_core_enabled(rq)))
 		return __pick_next_task(rq, prev, rf);
 
 	cpu = cpu_of(rq);
@@ -7301,7 +7301,7 @@ SYSCALL_DEFINE0(sched_yield)
 #if !defined(CONFIG_PREEMPTION) || defined(CONFIG_PREEMPT_DYNAMIC)
 int __sched __cond_resched(void)
 {
-	if (should_resched(0) && !irqs_disabled()) {
+	if (unlikely(should_resched(0)) && !irqs_disabled()) {
 		preempt_schedule_common();
 		return 1;
 	}
-- 
2.39.1

# Patch0150: 0150-select-core_sys_select-add-unlikely-branch-hint-on-r.patch

Subject: [PATCH] select: core_sys_select add unlikely branch hint on return path

Adding an unlikely() hint on the n < 0 comparison return path improves
run-time performance of the select() system call, the negative
value of n is very uncommon in normal select usage.

Benchmarking on an Debian based Intel(R) Core(TM) Ultra 9 285K with
a 6.15-rc1 kernel built with 14.2.0 using a select of 1000 file
descriptors with zero timeout shows a consistent call reduction from
258 ns down to 254 ns, which is a ~1.5% performance improvement.

Results based on running 25 tests with turbo disabled (to reduce clock
freq turbo changes), with 30 second run per test and comparing the number
of select() calls per second. The % standard deviation of the 25 tests
was 0.24%, so results are reliable.

Signed-off-by: Colin Ian King <colin.i.king@xxxxxxxxxx>
---
 fs/select.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/fs/select.c b/fs/select.c
index 0eaf3522abe9..9fb650d03d52 100644
--- a/fs/select.c
+++ b/fs/select.c
@@ -630,7 +630,7 @@ int core_sys_select(int n, fd_set __user *inp, fd_set __user *outp,
 	long stack_fds[SELECT_STACK_ALLOC/sizeof(long)];
 
 	ret = -EINVAL;
-	if (n < 0)
+	if (unlikely(n < 0))
 		goto out_nofds;
 
 	/* max_fds can increase, so grab it once to avoid race */
-- 
2.49.0

# Patch0157: scale-net-alloc.patch

diff --git a/include/net/sock.h b/include/net/sock.h
index 4e787285fc66..3e045f6eb6ee 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -1583,10 +1583,17 @@ static inline void sk_mem_charge(struct sock *sk, int size)
 
 static inline void sk_mem_uncharge(struct sock *sk, int size)
 {
+	int reclaimable, reclaim_threshold;
+
+	reclaim_threshold = 64 * 1024;
 	if (!sk_has_account(sk))
 		return;
 	sk_forward_alloc_add(sk, size);
-	sk_mem_reclaim(sk);
+	reclaimable = sk->sk_forward_alloc - sk_unused_reserved_mem(sk);
+	if (reclaimable > reclaim_threshold) {
+		reclaimable -= reclaim_threshold;
+		__sk_mem_reclaim(sk, reclaimable);
+	}
 }
 
 #if IS_ENABLED(CONFIG_PROVE_LOCKING) && IS_ENABLED(CONFIG_MODULES)

# Patch0158: 0158-clocksource-only-perform-extended-clocksource-checks.patch

Subject: [PATCH] clocksource: only perform extended clocksource checks for AMD
 systems

Signed-off-by: Colin Ian King <colin.king@xxxxxxxxxx>
---
 drivers/clocksource/acpi_pm.c | 7 +++++--
 1 file changed, 5 insertions(+), 2 deletions(-)

diff --git a/drivers/clocksource/acpi_pm.c b/drivers/clocksource/acpi_pm.c
index 82338773602c..d84f0e29452e 100644
--- a/drivers/clocksource/acpi_pm.c
+++ b/drivers/clocksource/acpi_pm.c
@@ -208,13 +208,16 @@ static int verify_pmtmr_rate(void)
 static int __init init_acpi_pm_clocksource(void)
 {
 	u64 value1, value2;
-	unsigned int i, j = 0;
+	unsigned int i, j = 0, checks = 1;
 
 	if (!pmtmr_ioport)
 		return -ENODEV;
 
+	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD)
+		checks = ACPI_PM_MONOTONICITY_CHECKS;
+
 	/* "verify" this timing source: */
-	for (j = 0; j < ACPI_PM_MONOTONICITY_CHECKS; j++) {
+	for (j = 0; j < checks; j++) {
 		udelay(100 * j);
 		value1 = clocksource_acpi_pm.read(&clocksource_acpi_pm);
 		for (i = 0; i < ACPI_PM_READ_CHECKS; i++) {
-- 
2.40.1

# Patch0164: 0164-KVM-VMX-make-vmx-init-a-late-init-call-to-get-to-ini.patch

Subject: [PATCH] KVM: VMX: make vmx_init a late init call to get to init process faster

Making vmx_init a late initcall improves QEMU kernel boot times to
get to the init process. Average of 100 boots, QEMU boot average
reduced from 0.776 seconds to 0.622 seconds (~19.8% faster) on
Alderlake i9-12900 and ~0.5% faster for non-QEMU UEFI boots.

Signed-off-by: Colin Ian King <colin.i.king@xxxxxxxxxx>
---
 arch/x86/kvm/vmx/vmx.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index bc6f0fea48b4..e671fbe70d5a 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -8767,4 +8767,4 @@ static int __init vmx_init(void)
 	kvm_x86_vendor_exit();
 	return r;
 }
-module_init(vmx_init);
+late_initcall(vmx_init);
-- 
2.42.0

# Patch0165: slack.patch

--- linux-6.5.1/init/init_task.c~	2023-09-02 07:13:30.000000000 +0000
+++ linux-6.5.1/init/init_task.c	2023-10-30 15:12:13.920976572 +0000
@@ -140,7 +140,7 @@
 	.journal_info	= NULL,
 	INIT_CPU_TIMERS(init_task)
 	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(init_task.pi_lock),
-	.timer_slack_ns = 50000, /* 50 usec default slack */
+	.timer_slack_ns = 50, /* 50 nsec default slack */
 	.thread_pid	= &init_struct_pid,
 	.thread_node	= LIST_HEAD_INIT(init_signals.thread_head),
 #ifdef CONFIG_AUDIT

# Patch0166: 0166-sched-fair-remove-upper-limit-on-cpu-number.patch

Subject: [PATCH] sched/fair: remove upper limit on cpu number

Signed-off-by: Colin Ian King <colin.i.king@xxxxxxxxxx>
---
 kernel/sched/fair.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 2048138ce54b..903ead0afacb 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -206,7 +206,7 @@ static inline void update_load_set(struct load_weight *lw, unsigned long w)
  */
 static unsigned int get_update_sysctl_factor(void)
 {
-	unsigned int cpus = min_t(unsigned int, num_online_cpus(), 8);
+	unsigned int cpus = num_online_cpus();
 	unsigned int factor;
 
 	switch (sysctl_sched_tunable_scaling) {
-- 
2.42.1

# Patch0167: 0167-net-sock-increase-default-number-of-_SK_MEM_PACKETS-.patch

Subject: [PATCH] net: sock: increase default number of _SK_MEM_PACKETS to 1024

scale these by a factor of 4 to improve socket performance

Signed-off-by: Colin Ian King <colin.i.king@xxxxxxxxxx>
---
 include/net/sock.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/include/net/sock.h b/include/net/sock.h
index 54ca8dcbfb43..9adc51e8085b 100644
--- a/include/net/sock.h
+++ b/include/net/sock.h
@@ -2887,7 +2887,7 @@ void sk_get_meminfo(const struct sock *sk, u32 *meminfo);
  * platforms.  This makes socket queueing behavior and performance
  * not depend upon such differences.
  */
-#define _SK_MEM_PACKETS		256
+#define _SK_MEM_PACKETS		1024
 #define _SK_MEM_OVERHEAD	SKB_TRUESIZE(256)
 #define SK_WMEM_MAX		(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)
 #define SK_RMEM_MAX		(_SK_MEM_OVERHEAD * _SK_MEM_PACKETS)
-- 
2.44.0

# Patch0173: 0173-cpuidle-psd-add-power-sleep-demotion-prevention-for-.patch

From f024054b3a97507df9b838e4840100c01d5f1d7d Mon Sep 17 00:00:00 2001
From: Colin Ian King <colin.i.king@intel.com>
Date: Tue, 3 Jun 2025 15:42:52 +0100
Subject: [PATCH] cpuidle: psd: power sleep demotion prevention for PCIe NVME I/O devices

Modern processors can drop into deep sleep states relatively quickly
to save power. However, coming out of deep sleep states takes a small
amount of time and this is detrimental to performance for I/O devices
such as fast PCIe NVME drives when servicing a completed I/O
transactions.

Testing with fio with read/write RAID0 PCIe NVME devices on various
modern SMP based systems (such as 96 thead Granite Rapids Xeon 6741P)
has shown that on 85-90% of read/write transactions issued on a CPU
are completed by the same CPU, so it makes some sense to prevent the
CPU from dropping into a deep sleep state to help reduce I/O handling
latency.

This commit introduces a simple, very lightweight and fast power sleep
demotion mechanism that provides the PCIe NVME driver a way to inform
the menu governor to prevent a CPU from going into a deep sleep when an
I/O operation is requested. While it is true that some I/Os may not
be serviced on the same CPU that issued the I/O request, the mechanism
does work well in the vast majority of I/O operations and there is very
small overhead with the sleep demotion prevention.

Test results on a 96 thread Xeon 6741P with a 6 way RAID0 PCIe NVME md
array. Tested with the NHM_C1_AUTO_DEMOTE bit set in the
MSR_PKG_CST_CONFIG_CONTROL (set in the BIOS).

Benchmarked with 4 I/O test cases from the Phoronix Test Suite v10.8.5,
each result is the average of 3 runs per test:

dbench 1.0.2:
  client count  original   patched   improvement
     1           1180.99   1320.52   11.8%
     6           3442.35   3606.76    4.7%
    12           5201.84   5441.22    4.6%
    48          11923.20  12638.30    6.0%
   128          14713.30  15314.10    4.1%
   256          15628.90  16208.00    3.7%

sqlite 2.2.0
  threads      original    patched  improvement
    16             7.93       7.19   -9.3%
    32            13.51      15.48   14.6%
    64            28.87      34.17   18.4%
    96            45.47      48.92    7.6%

iozone 1.9.6, 8GB file size:
I/O Size  Test original   patched   improvement
   4KB    Read  6838.45   6939.40     1.5%
   2MB    Read  7360.80   7464.94     1.4%
   4KB   Write    31.75     47.89    50.8%
   2MB   Write  1653.08   1817.86    10.6%

fio 2.1.0, Engine: Sync, Block Size 4K, I/O rate MB/sec
Test          Direct  Jobs  original  patched  improvement
Random Read       No   1        48.3     54.5   12.8%
Random Read       No   2        48.1     54.3   12.9%
Random Read       No   4        48.0     54.4   13.3%
Random Read       No   8        48.1     54.4   13.1%
Random Read       No  16        48.2     54.4   12.9%
Random Read       No  32        48.2     54.5   13.1%
Random Read       No  64        48.3     54.5   12.8%
Random Read       No  96        48.1     54.3   12.9%
Geometric Mean:                 48.2     54.4   13.0%

Random Read      Yes   1        50.7     56.0   10.5%
Random Read      Yes   2        50.7     55.9   10.3%
Random Read      Yes   4        50.7     55.9   10.3%
Random Read      Yes   8        50.7     56.0   10.5%
Random Read      Yes  16        50.7     55.9   10.3%
Random Read      Yes  32        50.5     55.9   10.7%
Random Read      Yes  64        50.6     55.8   10.3%
Random Read      Yes  96        50.7     55.8   10.1%
Geometric Mean:                 50.7     55.9   10.3%

Random Write      No   1        1488     1487   -0.1%
Random Write      No   2        1492     1483   -0.6%
Random Write      No   4        1485     1480   -0.3%
Random Write      No   8        1484     1479   -0.3%
Random Write      No  16        1490     1483   -0.5%
Random Write      No  32        1483     1490    0.5%
Random Write      No  64        1486     1485   -0.1%
Random Write      No  96        1476     1481    0.3%
Geometric Mean:                 1476     1477    0.1%

Random Write     Yes   1         298      335   12.4%
Random Write     Yes   2         297      335   12.8%
Random Write     Yes   4         298      334   12.1%
Random Write     Yes   8         299      337   12.7%
Random Write     Yes  16         296      332   12.2%
Random Write     Yes  32         298      331   11.1%
Random Write     Yes  64         297      335   12.8%
Random Write     Yes  96         298      335   12.4%
Geometric Mean:                  298      334   12.3%

Sequential Read   No   1        2622    2688     2.5%
Sequential Read   No   2        2647    2692     1.7%
Sequential Read   No   4        2634    2692     2.2%
Sequential Read   No   8        2661    2673     0.5%
Sequential Read   No  16        2673    2689     0.6%
Sequential Read   No  32        2667    2681     0.5%
Sequential Read   No  64        2640    2673     1.3%
Sequential Read   No  96        2635    2672     1.4%
Geometric Mean:                 2647    2683     1.1%

Sequential Read  Yes   1        87.4   105.0    20.1%
Sequential Read  Yes   2        87.5   104.0    18.9%
Sequential Read  Yes   4        87.3   105.0    20.3%
Sequential Read  Yes   8        87.2   105.0    20.4%
Sequential Read  Yes  16        87.4   105.0    20.1%
Sequential Read  Yes  32        86.5   105.0    21.4%
Sequential Read  Yes  64        87.0   104.0    19.5%
Sequential Read  Yes  96        87.2   105.0    20.4%
Geometric Mean:                 87.2   104.7    20.1%

Sequential Write  No   1        1868    1870     0.1%
Sequential Write  No   2        1879    1881     0.1%
Sequential Write  No   4        1877    1878     0.1%
Sequential Write  No   8        1887    1874    -0.7%
Sequential Write  No  16        1878    1885     0.4%
Sequential Write  No  32        1875    1878     0.2%
Sequential Write  No  64        1888    1879    -0.5%
Sequential Write  No  96        1892    1888    -0.5%
Geometric Mean:                 1881    1879    -0.2%

Sequential Write Yes   1         307     339    10.4%
Sequential Write Yes   2         306     341    11.4%
Sequential Write Yes   4         306     338    10.5%
Sequential Write Yes   8         307     339    10.4%
Sequential Write Yes  16         307     342    11.4%
Sequential Write Yes  32         307     337     9.8%
Sequential Write Yes  64         308     341    10.7%
Sequential Write Yes  96         306     341    11.4%
Geometric Mean:                  307     340    10.7%

For kernel builds, where all CPUs are fully loaded with a
small performance improvement based on the results of 5 kernel:
build test runs:

Kernel build time (geomean of 5 kernel builds, real time)
                  original   patched   improvement
Geometric Mean:      37.63s    37.39   0.8%
% Std.Dev.:           0.37%    0.31%

By default, CPU power sleep demotion blocking is set to run
for 1 ms on PCIe NVME I/O requests.

Signed-off-by: Colin Ian King <colin.i.king@intel.com>
---
 drivers/cpuidle/Kconfig          |  10 +++
 drivers/cpuidle/Makefile         |   1 +
 drivers/cpuidle/governors/menu.c |   4 ++
 drivers/cpuidle/psd.c            | 104 +++++++++++++++++++++++++++++++
 drivers/nvme/host/pci.c          |   3 +
 include/linux/cpuidle_psd.h      |  32 ++++++++++
 6 files changed, 154 insertions(+)
 create mode 100644 drivers/cpuidle/psd.c
 create mode 100644 include/linux/cpuidle_psd.h

diff --git a/drivers/cpuidle/Kconfig b/drivers/cpuidle/Kconfig
index cac5997dca50..b47b8e9e77b6 100644
--- a/drivers/cpuidle/Kconfig
+++ b/drivers/cpuidle/Kconfig
@@ -81,6 +81,16 @@ config HALTPOLL_CPUIDLE
 	 before halting in the guest (more efficient than polling in the
 	 host via halt_poll_ns for some scenarios).
 
+config CPU_IDLE_PSD
+	bool "prevent sleep demotion (PSD) for fast I/O devices"
+        default y
+        help
+         This option enables deferring of deep sleep states when a future
+         I/O based servicing event very probably going to happen in the very
+         near future, such as handling fast NVME device I/O. This reduces
+         uncessary transistions to deep idle sleep and reduces latency. This
+         provides the latency benefits of disabling deep sleep with the
+         power saving benefits of deep sleep when I/O is idle.
 endif
 
 config ARCH_NEEDS_CPU_IDLE_COUPLED
diff --git a/drivers/cpuidle/Makefile b/drivers/cpuidle/Makefile
index 1de9e92c5b0f..b219d488fe41 100644
--- a/drivers/cpuidle/Makefile
+++ b/drivers/cpuidle/Makefile
@@ -12,6 +12,7 @@ obj-$(CONFIG_DT_IDLE_STATES)		  += dt_idle_states.o
 obj-$(CONFIG_DT_IDLE_GENPD)		  += dt_idle_genpd.o
 obj-$(CONFIG_ARCH_HAS_CPU_RELAX)	  += poll_state.o
 obj-$(CONFIG_HALTPOLL_CPUIDLE)		  += cpuidle-haltpoll.o
+obj-$(CONFIG_CPU_IDLE_PSD)	  	  += psd.o
 
 ##################################################################################
 # ARM SoC drivers
diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index 52d5d26fc7c6..eb5f04def188 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -16,6 +16,7 @@
 #include <linux/tick.h>
 #include <linux/sched/stat.h>
 #include <linux/math64.h>
+#include <linux/cpuidle_psd.h>
 
 #include "gov.h"
 
@@ -224,6 +225,9 @@ static int menu_select(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		data->needs_update = 0;
 	}
 
+	if (have_prevent_sleep_demotion())
+		latency_req = 0;
+
 	/* Find the shortest expected idle interval. */
 	predicted_ns = get_typical_interval(data) * NSEC_PER_USEC;
 	if (predicted_ns > RESIDENCY_THRESHOLD_NS) {
diff --git a/drivers/cpuidle/psd.c b/drivers/cpuidle/psd.c
new file mode 100644
index 000000000000..92251178a0b9
--- /dev/null
+++ b/drivers/cpuidle/psd.c
@@ -0,0 +1,104 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ *  Copyright (C) 2025 Intel Corporation
+ *  Author: Colin Ian King <colin.king@intel.com>
+ *
+ *  Kernel Prevent Sleep Demotion (PSD)
+ */
+#include <linux/kernel.h>
+#include <linux/cpu.h>
+#include <linux/device.h>
+#include <linux/percpu.h>
+#include <linux/jiffies.h>
+#include <linux/cpuidle_psd.h>
+
+/* jiffies at which the lease times out */
+static DEFINE_PER_CPU(unsigned long, psd_timeout);
+static int psd_cpu_lat_timeout_jiffies;
+
+/*
+ * A note about the use of the current cpu versus preemption.
+ *
+ * The use of have_prevent_sleep_demotion() is inside local
+ * power management code, and are pinned to that cpu already.
+ *
+ * On the "set" side, interrupt level code is obviously also fully
+ * migration-race free.
+ *
+ * All other cases are exposed to a migration-race.
+ *
+ * The goal of prevent sleep demotion is statistical rather than
+ * deterministic, e.g. on average the CPU that hits event X will go
+ * towards Y more often than not, and the impact of being wrong is a
+ * bit of extra power potentially for some short durations.
+ * Weighted against the costs in performance and complexity of dealing
+ * with the race, the race condition is acceptable.
+ *
+ * The second known race is where interrupt context might set a
+ * psd time in the middle of process context setting a different but
+ * psd smaller time, with the result that process context will win
+ * incorrectly, and the actual psd time will be less than expected,
+ * but still non-zero. Here also the cost of dealing with the race
+ * is outweight with the limited impact.
+ *
+ * The use of timings in jiffies is intentional, it is lightweight
+ * read and very fast. While it mau seem that using finer resolution
+ * timings is preferable, the expense is too high on I/O fast paths
+ * when preventing sleep demotions via prevent_sleep_demotion.
+ *
+ */
+int have_prevent_sleep_demotion(void)
+{
+	if (likely(psd_cpu_lat_timeout_jiffies)) {
+		int cpu = raw_smp_processor_id();
+
+		if (time_before(jiffies, per_cpu(psd_timeout, cpu)))
+			return 1;
+
+		/* keep the stored time value close to current */
+		per_cpu(psd_timeout, cpu) = jiffies;
+	}
+	return 0;
+}
+EXPORT_SYMBOL_GPL(have_prevent_sleep_demotion);
+
+void prevent_sleep_demotion(void)
+{
+	if (likely(psd_cpu_lat_timeout_jiffies)) {
+		const unsigned long next_jiffies = jiffies + psd_cpu_lat_timeout_jiffies;
+		const int cpu = raw_smp_processor_id();
+
+		/*  need to round up an extra jiffie */
+		if (time_before(per_cpu(psd_timeout, cpu), next_jiffies))
+			per_cpu(psd_timeout, cpu) = next_jiffies;
+	}
+}
+EXPORT_SYMBOL_GPL(prevent_sleep_demotion);
+
+static int psd_msecs_to_jiffies(const int msec)
+{
+	int ret = msecs_to_jiffies(msec);
+
+	return msec > 0 && ret == 0 ? 1 : ret;
+}
+
+static __init int prevent_sleep_demotion_init(void)
+{
+	struct device *dev_root = bus_get_dev_root(&cpu_subsys);
+	unsigned int cpu;
+
+	if (!dev_root)
+		return -1;
+
+	psd_cpu_lat_timeout_jiffies = psd_msecs_to_jiffies(PSD_NVME_DISK_MSEC);
+
+	pr_info("cpuidle-psd: using %d msec (%d jiffies) for idle timing\n",
+		PSD_NVME_DISK_MSEC, psd_cpu_lat_timeout_jiffies);
+
+	for_each_possible_cpu(cpu)
+		per_cpu(psd_timeout, cpu) = jiffies;
+
+	return 0;
+}
+
+late_initcall(prevent_sleep_demotion_init);
diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c
index e0bfe04a2bc2..2bfee1a5bf78 100644
--- a/drivers/nvme/host/pci.c
+++ b/drivers/nvme/host/pci.c
@@ -28,6 +28,7 @@
 #include <linux/io-64-nonatomic-hi-lo.h>
 #include <linux/sed-opal.h>
 #include <linux/pci-p2pdma.h>
+#include <linux/cpuidle_psd.h>
 
 #include "trace.h"
 #include "nvme.h"
@@ -1044,6 +1045,7 @@ static blk_status_t nvme_queue_rq(struct blk_mq_hw_ctx *hctx,
 	ret = nvme_prep_rq(dev, req);
 	if (unlikely(ret))
 		return ret;
+	prevent_sleep_demotion();
 	spin_lock(&nvmeq->sq_lock);
 	nvme_sq_copy_cmd(nvmeq, &iod->cmd);
 	nvme_write_sq_db(nvmeq, bd->last);
@@ -1089,6 +1091,7 @@ static void nvme_queue_rqs(struct rq_list *rqlist)
 	struct nvme_queue *nvmeq = NULL;
 	struct request *req;
 
+	prevent_sleep_demotion();
 	while ((req = rq_list_pop(rqlist))) {
 		if (nvmeq && nvmeq != req->mq_hctx->driver_data)
 			nvme_submit_cmds(nvmeq, &submit_list);
diff --git a/include/linux/cpuidle_psd.h b/include/linux/cpuidle_psd.h
new file mode 100644
index 000000000000..5bcca5d53675
--- /dev/null
+++ b/include/linux/cpuidle_psd.h
@@ -0,0 +1,32 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ *  Copyright (C) 2025 Intel Corporation
+ *  Author: Colin Ian King <colin.king@intel.com>
+ *
+ *  Kernel prevent sleep demotion infrastructructure
+ */
+#ifndef _LINUX_CPUIDLE_PSD_H
+#define _LINUX_CPUIDLE_PSD_H
+
+/* duration of sleep demotion for PCIe NVME disks in msec */
+#define PSD_NVME_DISK_MSEC		(1)
+
+/* API prototypes */
+#ifdef CONFIG_CPU_IDLE_PSD
+
+extern void prevent_sleep_demotion(void);
+extern int have_prevent_sleep_demotion(void);
+
+#else
+
+static inline void prevent_sleep_demotion(void)
+{
+}
+
+static inline int have_prevent_sleep_demotion(void)
+{
+	return 0;
+}
+#endif
+
+#endif
-- 
2.49.0

# Patch0174: 0174-memcg-increase-MEMCG_CHARGE_BATCH-to-128.patch

Subject: [PATCH] memcg: increase MEMCG_CHARGE_BATCH to 128

MEMCG_CHARGE_BATCH was last changed to 64 back in 2022, systems have
grown in memory and speed and it's useful to increase this to 128.

Benchmarking the stress-ng mmap stressor shows a performance improvement
of ~7.4% and malloc stressor by 2.8%, tested on an Ultra 9 285K with
turbo disabled to avoid test result jitter.

Signed-off-by: Colin Ian King <colin.i.king@xxxxxxxxxx>
---
 include/linux/memcontrol.h | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index 6e74b8254d9b..a47c977bea18 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -316,7 +316,7 @@ struct mem_cgroup {
  * TODO: maybe necessary to use big numbers in big irons or dynamic based of the
  * workload.
  */
-#define MEMCG_CHARGE_BATCH 64U
+#define MEMCG_CHARGE_BATCH 128U
 
 extern struct mem_cgroup *root_mem_cgroup;
 
-- 
2.49.0
